\documentclass{article}

\usepackage{graphicx}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{url}
\usepackage[usenames]{color}

\newcommand{\figref}[1]{Figure~\ref{#1}}

\pagestyle{empty} \addtolength{\textwidth}{1.0in}
\addtolength{\textheight}{0.5in} \addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\newcommand{\ruleskip}{\bigskip\hrule\bigskip}
\newcommand{\nodify}[1]{{\sc #1}} \newcommand{\points}[1]{{\textbf{[#1
points]}}}

\newcommand{\bitem}{\begin{list}{$\bullet$}%
{\setlength{\itemsep}{0pt}\setlength{\topsep}{0pt}%
\setlength{\rightmargin}{0pt}}} \newcommand{\eitem}{\end{list}}

\newcommand{\G}{\mathcal{G}}
\newcommand{\E}{\mathbb{E}}

%\newcommand{\bE}{\mbox{\boldmath $E$}}
%\newcommand{\be}{\mbox{\boldmath $e$}}
%\newcommand{\bU}{\mbox{\boldmath $U$}}
%\newcommand{\bu}{\mbox{\boldmath $u$}}
%\newcommand{\bQ}{\mbox{\boldmath $Q$}}
%\newcommand{\bq}{\mbox{\boldmath $q$}}
%\newcommand{\bX}{\mbox{\boldmath $X$}}
%\newcommand{\bY}{\mbox{\boldmath $Y$}}
%\newcommand{\bZ}{\mbox{\boldmath $Z$}}
%\newcommand{\bx}{\mbox{\boldmath $x$}}
%\newcommand{\by}{\mbox{\boldmath $y$}}
%\newcommand{\bz}{\mbox{\boldmath $z$}}

\newcommand{\true}{\mbox{true}}
\newcommand{\Parents}{\mbox{Parents}}

\newcommand{\ww}{{\bf w}}
\newcommand{\xx}{{\bf x}}
\newcommand{\yy}{{\bf y}}
\newcommand{\real}{\ensuremath{\mathbb{R}}}


\newcommand{\eat}[1]{}

\newcommand{\CInd}[3]{({#1} \perp {#2} \mid {#3})}
\newcommand{\Ind}[2]{({#1} \perp {#2})}

\setlength{\parindent}{0pt} \setlength{\parskip}{0.5ex}

\begin{document}

\pagestyle{myheadings} \markboth{}{DS-GA-1005/CSCI-GA.2569 Problem Set 5}

{\LARGE
\begin{center}Inference and Representation, Fall 2016\end{center}
}

{\Large
Problem Set 5: EM \& Factor Analysis
}


{\bf Due: Monday, October 24, 2016 at 3pm (as a PDF document uploaded in
  Gradescope.)}\\

{\em {\bf Important:} See problem set policy on the course web site.}
\ruleskip

%\vspace{0.2in}

\begin{enumerate}

\item \emph{Non-negative Matrix Factorization (NMF)} [Lee and Seung'99] is an alternative to PCA when 
data and factors can be cast as non-negative. We seek to factorize the $N \times p$ data matrix $\mathbf{X}$ as
\begin{equation}
\mathbf{X} \approx \mathbf{ W ~H}~,
\end{equation}
where $\mathbf{W}$ is $N \times r$ and $\mathbf{H}$ is $r \times p$, with $r \leq \max(N,p)$, and 
we assume that $x_{ij}, w_{ik}, h_{kj} \geq 0$.
\begin{enumerate}
\item Suppose that $x_{ij} \in \mathbb{N}$. If we model each random variable $x_{ij}$ as a Poisson 
random variable with mean $(WH)_{ij}$, show that the log-likelihood of the model is (up to a constant) 
\begin{equation}
\label{bla}
\mathcal{L}(\mathbf{W}, \mathbf{H}) = \sum_{i, j} [x_{ij} \log( (WH)_{ij} ) - (WH)_{ij}] ~.
\end{equation}

\end{enumerate}

 The following alternating algorithm (Lee, Seung, '01) converges to a local maximum of $\mathcal{L}(\mathbf{W}, \mathbf{H})$:
\begin{eqnarray}
\label{ble}
w_{ik} &\leftarrow& w_{ik} \frac{\sum_j h_{kj}x_{ij} / ( WH)_{ij}}{\sum_j h_{kj}}~,\\
h_{kj} &\leftarrow& h_{kj} \frac{\sum_i w_{ik}x_{ij} / ( WH)_{ij}}{\sum_j w_{kj}}~,.
\end{eqnarray}
We shall study this algorithm and prove its correctness. 

A function $g(x,y)$ is said to minorize a function $f(x)$ if 
$$\forall~(x,y)~,~g(x,y) \leq f(x)~,~g(x,x) = f(x)~.$$
\begin{enumerate}
\setcounter{enumi}{1}
\item Show that under the update 
$$x^{t+1} = \arg\max_x g(x,x^t)$$
the sequence $f_t = f(x^t)$ is non-decreasing. 
\item Using concavity of the logarithm, show that for any set of $r$ values $y_k \geq 0$ 
and $0 \leq c_k \leq 1$ with $\sum_{k \leq r} c_k = 1$, 
$$\log \left( \sum_{k \leq r} y_k \right) \geq \sum_{k \leq r} c_k \log( y_k / c_k)~.$$
\item Deduce that 
$$\log \left( \sum_{k \leq r} w_{ik} h_{kj} \right) \geq \sum_{k \leq r} c_{kij} \log( w_{ik} h_{kj} / c_{kij})~,$$
where $c_{kij} = \frac{w_{ik}^t h_{kj}^t}{ \sum_{k' \leq r} w_{ik'}^t h_{k'j}^t} $ and $t$ is the current iteration.
\item Ignoring constants, show that 
$$g(\mathbf{W}, \mathbf{H}; \mathbf{W}^t, \mathbf{H}^t) = \sum_{i,j,k} [x_{ij} c_{kij} (\log w_{ik} + \log h_{kj}) - w_{ik}h_{kj} ]$$
minorizes $\mathcal{L}(\mathbf{W}, \mathbf{H})$. 
\item Finally, derive the update steps (\ref{ble}) by setting to zero the partial derivatives of $g$.
\end{enumerate}

\item \emph{Factor Analysis, Covariance and Correlation}. 
Recall that the covariance and correlation of two random variables $X_i$, $X_j$ defined respectively as 
$$\sigma_{i,j} = \E( (X_i - \E X_i)(X_j - \E X_j))~,~\tilde{\sigma}_{i,j} = \frac{\sigma_{i,j}}{\sqrt{\sigma_{i,i} \sigma_{j,j}}}~.$$
\begin{enumerate}
\item Show that $-1 \leq \tilde{\sigma}_{ij} \leq 1$.
\item Show the relationship between Factor Analysis applied to the covariance matrix $\Sigma$ of $X$ and the corresponding Factor Analysis applied to the correlation matrix $\tilde{\Sigma}$ of $X$. Do you obtain the same relationship than Principal Component Analysis? 
\item Construct an example (A) with three random variables exhibiting some correlation, such that the leading principal component fails to detect that correlation but the leading factor analysis direction does, and an example (B) where the detected principal component aligns better with the underlying factor than the leading factor analysis direction.

\end{enumerate}



\item 
% From Berkeley, A_hw5_Fa11.tex
%\begin{center}{\bf \large  HMM with mixture model emissions}\end{center}

A common modification of the hidden Markov model involves using mixture models for the
emission probabilities $p(\yy_t | q_t)$, where $q_t$ refers to the state for time $t$ and $\yy_t$ to the observation for time $t$.

Suppose that ${\bf y}_t \in \real^n$ and that the emission distribution is given by a mixture of Gaussians for each value of the state. To be concrete, suppose that the $q_t$ can take $K$ discrete states and each mixture has $M$ components. Then,
\[
p({\bf y}_t \mid q_t) = \sum_{j=1}^M b_{q_t j}
\left(\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_{q_t j}|^{\frac{1}{2}}}
\exp\left\{-\frac{1}{2}({\bf y}_t-\mu_{q_t j})^T\Sigma_{q_t j}^{-1}({\bf y}_t-\mu_{q_t j})
\right\}\right)
\]
where ${\bf b}_i \in [0,1]^M$ denotes the mixing weights for state $i$ ($\sum_{j=1}^M b_{ij} = 1$ for $i=1,\ldots K$), $\mu_{ij} \in  \real^n$ and $\Sigma_{ij} \in  \real^{n\times n}$.

Let $\pi \in \real^K$ be the probability distribution for the initial state $q_0$,
and $A \in \real^{K \times K}$ be the transition matrix of the $q_t$'s.
In this problem you will derive an EM algorithm for learning the parameters $\{ b_{ij}, \mu_{ij}, \Sigma_{ij} \}$ and $A, \pi$.

\begin{enumerate}
\item The EM algorithm is substantially simpler if you introduce auxiliary variables $z_t\in \{1, \ldots, M\}$ denoting which mixture component the $t$'th observation is drawn from.

Draw the graphical model for this modified HMM, identifying
  clearly the additional latent variables that are needed.
\item Write the expected complete log likelihood for the model
  and identify the expectations that you need to compute in the E
  step. {\em Show all steps of your derivation.}

\item Give an algorithm for computing the E step.

{\em Hint: Reduce the inference problem to something you know how to do, such as sum-product belief propagation in tree-structured pairwise MRFs.}

\item Write down the equations that implement the M step.
\end{enumerate}


\end{enumerate}

\end{document}
